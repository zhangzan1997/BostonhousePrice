{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目录已存在\n",
      "第 1 折\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 293 samples, validate on 97 samples\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 289.5245\n",
      " - 2s - loss: 422.9639 - val_loss: 289.1825\n",
      "Epoch 2/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 65.3873\n",
      " - 0s - loss: 179.9990 - val_loss: 67.2290\n",
      "Epoch 3/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 38.0276\n",
      " - 0s - loss: 45.7393 - val_loss: 38.5972\n",
      "Epoch 4/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 28.6632\n",
      " - 0s - loss: 25.7303 - val_loss: 27.7968\n",
      "Epoch 5/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 24.6792\n",
      " - 0s - loss: 19.0637 - val_loss: 23.4448\n",
      "Epoch 6/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 21.8012\n",
      " - 0s - loss: 15.9357 - val_loss: 20.0886\n",
      "Epoch 7/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 20.3254\n",
      " - 0s - loss: 14.0237 - val_loss: 17.5030\n",
      "Epoch 8/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 19.1279\n",
      " - 0s - loss: 12.4488 - val_loss: 15.9633\n",
      "Epoch 9/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 18.0035\n",
      " - 0s - loss: 11.2566 - val_loss: 14.7196\n",
      "Epoch 10/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 17.2066\n",
      " - 0s - loss: 10.3098 - val_loss: 14.0316\n",
      "Epoch 11/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 16.9720\n",
      " - 0s - loss: 9.6757 - val_loss: 12.9891\n",
      "Epoch 12/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 16.1400\n",
      " - 0s - loss: 8.9802 - val_loss: 12.8222\n",
      "Epoch 13/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 15.8240\n",
      " - 0s - loss: 8.4839 - val_loss: 11.6329\n",
      "Epoch 14/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 15.0829\n",
      " - 0s - loss: 7.9413 - val_loss: 11.2915\n",
      "Epoch 15/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 15.5485\n",
      " - 0s - loss: 7.6986 - val_loss: 11.8522\n",
      "Epoch 16/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 14.8593\n",
      "目前最优test_mse为14.859\n",
      " - 0s - loss: 7.4554 - val_loss: 10.4485\n",
      "Epoch 17/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 14.5707\n",
      "目前最优test_mse为14.571\n",
      " - 0s - loss: 7.1502 - val_loss: 10.3219\n",
      "Epoch 18/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 14.7801\n",
      " - 0s - loss: 6.7962 - val_loss: 9.8795\n",
      "Epoch 19/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 14.3218\n",
      "目前最优test_mse为14.322\n",
      " - 0s - loss: 6.6373 - val_loss: 9.7869\n",
      "Epoch 20/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 14.6263\n",
      " - 0s - loss: 6.5248 - val_loss: 9.4980\n",
      "Epoch 21/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 14.7568\n",
      " - 0s - loss: 6.3801 - val_loss: 10.5843\n",
      "Epoch 22/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 14.5002\n",
      " - 0s - loss: 6.2778 - val_loss: 9.5368\n",
      "Epoch 23/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 13.8193\n",
      "目前最优test_mse为13.819\n",
      " - 0s - loss: 6.1873 - val_loss: 9.0811\n",
      "Epoch 24/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 14.3705\n",
      " - 0s - loss: 5.9572 - val_loss: 9.2225\n",
      "Epoch 25/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 14.2869\n",
      " - 0s - loss: 5.6368 - val_loss: 8.6446\n",
      "Epoch 26/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 14.3799\n",
      " - 0s - loss: 5.7738 - val_loss: 8.6483\n",
      "Epoch 27/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 13.7181\n",
      "目前最优test_mse为13.718\n",
      " - 0s - loss: 5.4655 - val_loss: 8.8516\n",
      "Epoch 28/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 13.8104\n",
      " - 0s - loss: 5.4452 - val_loss: 8.7762\n",
      "Epoch 29/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 14.2208\n",
      " - 0s - loss: 5.3291 - val_loss: 8.8823\n",
      "Epoch 30/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 13.9338\n",
      " - 0s - loss: 5.3459 - val_loss: 8.1327\n",
      "Epoch 31/500\n",
      "102/102 [==============================] - 0s 57us/sample - loss: 14.3005\n",
      " - 0s - loss: 5.1792 - val_loss: 8.2462\n",
      "Epoch 32/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 14.0490\n",
      " - 0s - loss: 5.2990 - val_loss: 8.0708\n",
      "Epoch 33/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 14.0242\n",
      " - 0s - loss: 5.0399 - val_loss: 7.8988\n",
      "Epoch 34/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 13.5150\n",
      "目前最优test_mse为13.515\n",
      " - 0s - loss: 5.0690 - val_loss: 8.2063\n",
      "Epoch 35/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 14.2553\n",
      " - 0s - loss: 4.8340 - val_loss: 7.7606\n",
      "Epoch 36/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 14.2151\n",
      " - 0s - loss: 4.9082 - val_loss: 7.8479\n",
      "Epoch 37/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 13.6515\n",
      " - 0s - loss: 4.9176 - val_loss: 7.8031\n",
      "Epoch 38/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 14.1281\n",
      " - 0s - loss: 4.7978 - val_loss: 7.6394\n",
      "Epoch 39/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 14.5124\n",
      " - 0s - loss: 4.9588 - val_loss: 7.8079\n",
      "Epoch 40/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 13.9337\n",
      " - 0s - loss: 4.7474 - val_loss: 7.8641\n",
      "Epoch 41/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 14.0835\n",
      " - 0s - loss: 4.5178 - val_loss: 7.8906\n",
      "Epoch 42/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 13.4671\n",
      "目前最优test_mse为13.467\n",
      " - 0s - loss: 4.5292 - val_loss: 7.3035\n",
      "Epoch 43/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 14.0464\n",
      " - 0s - loss: 4.4263 - val_loss: 7.4308\n",
      "Epoch 44/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 13.8406\n",
      " - 0s - loss: 4.4650 - val_loss: 7.4893\n",
      "Epoch 45/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 14.2239\n",
      " - 0s - loss: 4.4756 - val_loss: 7.7491\n",
      "Epoch 46/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 14.7089\n",
      " - 0s - loss: 4.6754 - val_loss: 8.2821\n",
      "Epoch 47/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 13.5007\n",
      " - 0s - loss: 4.2010 - val_loss: 7.1433\n",
      "Epoch 48/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 13.6278\n",
      " - 0s - loss: 4.2667 - val_loss: 7.0576\n",
      "Epoch 49/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 13.4466\n",
      "目前最优test_mse为13.447\n",
      " - 0s - loss: 4.2586 - val_loss: 6.9329\n",
      "Epoch 50/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 13.9208\n",
      " - 0s - loss: 4.1281 - val_loss: 7.2091\n",
      "Epoch 51/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 13.3728\n",
      "目前最优test_mse为13.373\n",
      " - 0s - loss: 4.1294 - val_loss: 6.9947\n",
      "Epoch 52/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 13.3230\n",
      "目前最优test_mse为13.323\n",
      " - 0s - loss: 4.3596 - val_loss: 7.0589\n",
      "Epoch 53/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 13.5014\n",
      " - 0s - loss: 4.1175 - val_loss: 6.8995\n",
      "Epoch 54/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 13.4402\n",
      " - 0s - loss: 4.0918 - val_loss: 6.7342\n",
      "Epoch 55/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 14.0314\n",
      " - 0s - loss: 3.9921 - val_loss: 7.0098\n",
      "Epoch 56/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 13.8483\n",
      " - 0s - loss: 4.0442 - val_loss: 6.8559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 13.7018\n",
      " - 0s - loss: 4.0435 - val_loss: 6.7435\n",
      "Epoch 58/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 13.0387\n",
      "目前最优test_mse为13.039\n",
      " - 0s - loss: 3.8992 - val_loss: 6.5655\n",
      "Epoch 59/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 13.0493\n",
      " - 0s - loss: 3.7725 - val_loss: 6.6575\n",
      "Epoch 60/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 13.1538\n",
      " - 0s - loss: 4.1201 - val_loss: 6.3613\n",
      "Epoch 61/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 13.1754\n",
      " - 0s - loss: 3.8443 - val_loss: 6.6622\n",
      "Epoch 62/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 13.2828\n",
      " - 0s - loss: 3.8246 - val_loss: 6.3355\n",
      "Epoch 63/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 12.9634\n",
      "目前最优test_mse为12.963\n",
      " - 0s - loss: 3.8352 - val_loss: 6.3348\n",
      "Epoch 64/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 13.2627\n",
      " - 0s - loss: 3.7973 - val_loss: 6.4125\n",
      "Epoch 65/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 13.2761\n",
      " - 0s - loss: 3.6269 - val_loss: 6.7936\n",
      "Epoch 66/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 13.2336\n",
      " - 0s - loss: 3.6412 - val_loss: 6.3006\n",
      "Epoch 67/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 14.0078\n",
      " - 0s - loss: 3.7804 - val_loss: 6.9287\n",
      "Epoch 68/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 13.5354\n",
      " - 0s - loss: 3.5984 - val_loss: 6.5759\n",
      "Epoch 69/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 12.9638\n",
      " - 0s - loss: 3.6167 - val_loss: 6.3215\n",
      "Epoch 70/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 12.8562\n",
      "目前最优test_mse为12.856\n",
      " - 0s - loss: 3.6175 - val_loss: 6.3118\n",
      "Epoch 71/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 13.1191\n",
      " - 0s - loss: 3.3931 - val_loss: 6.3647\n",
      "Epoch 72/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 13.6883\n",
      " - 0s - loss: 3.5309 - val_loss: 6.6859\n",
      "Epoch 73/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 13.7038\n",
      " - 0s - loss: 3.6515 - val_loss: 6.6695\n",
      "Epoch 74/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 13.0377\n",
      " - 0s - loss: 3.3888 - val_loss: 6.0580\n",
      "Epoch 75/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 13.0621\n",
      " - 0s - loss: 3.4470 - val_loss: 6.1753\n",
      "Epoch 76/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 13.1608\n",
      " - 0s - loss: 3.2868 - val_loss: 6.3480\n",
      "Epoch 77/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 13.4954\n",
      " - 0s - loss: 3.3071 - val_loss: 6.7883\n",
      "Epoch 78/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 13.5082\n",
      " - 0s - loss: 3.4958 - val_loss: 6.1790\n",
      "Epoch 79/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 12.8575\n",
      " - 0s - loss: 3.5576 - val_loss: 6.2752\n",
      "Epoch 80/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 13.2987\n",
      " - 0s - loss: 3.2752 - val_loss: 6.0671\n",
      "Epoch 81/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 13.3102\n",
      " - 0s - loss: 3.3291 - val_loss: 6.2184\n",
      "Epoch 82/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 13.0003\n",
      " - 0s - loss: 3.2718 - val_loss: 6.0011\n",
      "Epoch 83/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.8222\n",
      "目前最优test_mse为12.822\n",
      " - 0s - loss: 3.2602 - val_loss: 6.1983\n",
      "Epoch 84/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 12.5004\n",
      "目前最优test_mse为12.500\n",
      " - 0s - loss: 3.3582 - val_loss: 6.0378\n",
      "Epoch 85/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 12.5609\n",
      " - 0s - loss: 3.2514 - val_loss: 6.1364\n",
      "Epoch 86/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 12.5965\n",
      " - 0s - loss: 3.1276 - val_loss: 5.9577\n",
      "Epoch 87/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 13.0219\n",
      " - 0s - loss: 3.2236 - val_loss: 6.5886\n",
      "Epoch 88/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 12.7422\n",
      " - 0s - loss: 3.2675 - val_loss: 5.7347\n",
      "Epoch 89/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 13.0380\n",
      " - 0s - loss: 2.9911 - val_loss: 6.6348\n",
      "Epoch 90/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 12.7210\n",
      " - 0s - loss: 3.0592 - val_loss: 6.1510\n",
      "Epoch 91/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 12.4520\n",
      "目前最优test_mse为12.452\n",
      " - 0s - loss: 3.0961 - val_loss: 5.6454\n",
      "Epoch 92/500\n",
      "102/102 [==============================] - 0s 49us/sample - loss: 12.6443\n",
      " - 0s - loss: 3.3365 - val_loss: 5.9905\n",
      "Epoch 93/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 13.1793\n",
      " - 0s - loss: 3.1176 - val_loss: 6.3320\n",
      "Epoch 94/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 12.3981\n",
      "目前最优test_mse为12.398\n",
      " - 0s - loss: 3.0164 - val_loss: 5.6921\n",
      "Epoch 95/500\n",
      "102/102 [==============================] - 0s 59us/sample - loss: 13.4414\n",
      " - 0s - loss: 2.8272 - val_loss: 6.4347\n",
      "Epoch 96/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.4502\n",
      " - 0s - loss: 2.8273 - val_loss: 5.7761\n",
      "Epoch 97/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 13.1769\n",
      " - 0s - loss: 2.9756 - val_loss: 6.0077\n",
      "Epoch 98/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.4075\n",
      " - 0s - loss: 2.9211 - val_loss: 5.6289\n",
      "Epoch 99/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.3848\n",
      "目前最优test_mse为12.385\n",
      " - 0s - loss: 2.8431 - val_loss: 5.8532\n",
      "Epoch 100/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.5313\n",
      " - 0s - loss: 2.8626 - val_loss: 5.7100\n",
      "Epoch 101/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.6075\n",
      " - 0s - loss: 2.9497 - val_loss: 5.7632\n",
      "Epoch 102/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 12.3134\n",
      "目前最优test_mse为12.313\n",
      " - 0s - loss: 2.9761 - val_loss: 5.6945\n",
      "Epoch 103/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.6223\n",
      " - 0s - loss: 2.8194 - val_loss: 6.1731\n",
      "Epoch 104/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.4109\n",
      " - 0s - loss: 2.9545 - val_loss: 5.7921\n",
      "Epoch 105/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.6097\n",
      " - 0s - loss: 2.7754 - val_loss: 5.6823\n",
      "Epoch 106/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.7900\n",
      " - 0s - loss: 2.7813 - val_loss: 6.1609\n",
      "Epoch 107/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.6750\n",
      " - 0s - loss: 2.8450 - val_loss: 5.5726\n",
      "Epoch 108/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.5616\n",
      " - 0s - loss: 2.7646 - val_loss: 5.9803\n",
      "Epoch 109/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.6000\n",
      " - 0s - loss: 2.6949 - val_loss: 5.7128\n",
      "Epoch 110/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 12.6278\n",
      " - 0s - loss: 2.7202 - val_loss: 5.6315\n",
      "Epoch 111/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.1966\n",
      "目前最优test_mse为12.197\n",
      " - 0s - loss: 2.6037 - val_loss: 5.6557\n",
      "Epoch 112/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.3882\n",
      " - 0s - loss: 2.5919 - val_loss: 5.7029\n",
      "Epoch 113/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 12.0945\n",
      "目前最优test_mse为12.095\n",
      " - 0s - loss: 2.5467 - val_loss: 5.5628\n",
      "Epoch 114/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.8952\n",
      " - 0s - loss: 2.5458 - val_loss: 5.8207\n",
      "Epoch 115/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.4019\n",
      " - 0s - loss: 2.4872 - val_loss: 5.5250\n",
      "Epoch 116/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.8252\n",
      " - 0s - loss: 2.6515 - val_loss: 5.8421\n",
      "Epoch 117/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.9324\n",
      " - 0s - loss: 2.6831 - val_loss: 5.6813\n",
      "Epoch 118/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 12.3499\n",
      " - 0s - loss: 2.7099 - val_loss: 5.6744\n",
      "Epoch 119/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 0s 98us/sample - loss: 12.6291\n",
      " - 0s - loss: 2.5511 - val_loss: 5.7293\n",
      "Epoch 120/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.6615\n",
      " - 0s - loss: 2.7932 - val_loss: 5.8565\n",
      "Epoch 121/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.3387\n",
      " - 0s - loss: 2.4959 - val_loss: 5.5130\n",
      "Epoch 122/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 12.6982\n",
      " - 0s - loss: 2.4533 - val_loss: 5.8199\n",
      "Epoch 123/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.4669\n",
      " - 0s - loss: 2.4019 - val_loss: 5.4386\n",
      "Epoch 124/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.5711\n",
      " - 0s - loss: 2.3630 - val_loss: 5.7555\n",
      "Epoch 125/500\n",
      "102/102 [==============================] - 0s 108us/sample - loss: 12.1919\n",
      " - 0s - loss: 2.4118 - val_loss: 5.3895\n",
      "Epoch 126/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 12.6167\n",
      " - 0s - loss: 2.5565 - val_loss: 5.7452\n",
      "Epoch 127/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 12.2400\n",
      " - 0s - loss: 2.4404 - val_loss: 5.6581\n",
      "Epoch 128/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.3929\n",
      " - 0s - loss: 2.3260 - val_loss: 5.4601\n",
      "Epoch 129/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.4592\n",
      " - 0s - loss: 2.3730 - val_loss: 5.4046\n",
      "Epoch 130/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.5956\n",
      " - 0s - loss: 2.3583 - val_loss: 5.7744\n",
      "Epoch 131/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.6293\n",
      " - 0s - loss: 2.3644 - val_loss: 5.4695\n",
      "Epoch 132/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.2154\n",
      " - 0s - loss: 2.2920 - val_loss: 5.6615\n",
      "Epoch 133/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.4395\n",
      " - 0s - loss: 2.2879 - val_loss: 5.5165\n",
      "Epoch 134/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 12.3650\n",
      " - 0s - loss: 2.3887 - val_loss: 5.5331\n",
      "Epoch 135/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.6534\n",
      " - 0s - loss: 2.1037 - val_loss: 5.9863\n",
      "Epoch 136/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.5888\n",
      " - 0s - loss: 2.2645 - val_loss: 5.9672\n",
      "Epoch 137/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 12.3522\n",
      " - 0s - loss: 2.0779 - val_loss: 5.4559\n",
      "Epoch 138/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 12.3547\n",
      " - 0s - loss: 2.0536 - val_loss: 5.6423\n",
      "Epoch 139/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 12.4627\n",
      " - 0s - loss: 2.0750 - val_loss: 5.5281\n",
      "Epoch 140/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.4461\n",
      " - 0s - loss: 2.0998 - val_loss: 5.7576\n",
      "Epoch 141/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.2097\n",
      " - 0s - loss: 2.0938 - val_loss: 5.4842\n",
      "Epoch 142/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.6559\n",
      " - 0s - loss: 2.0346 - val_loss: 5.6366\n",
      "Epoch 143/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.3177\n",
      " - 0s - loss: 2.0533 - val_loss: 5.4114\n",
      "Epoch 144/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.1835\n",
      " - 0s - loss: 2.0389 - val_loss: 5.4361\n",
      "Epoch 145/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.2975\n",
      " - 0s - loss: 2.0259 - val_loss: 5.5552\n",
      "Epoch 146/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.2484\n",
      " - 0s - loss: 1.8856 - val_loss: 5.4265\n",
      "Epoch 147/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.2786\n",
      " - 0s - loss: 1.8968 - val_loss: 5.4777\n",
      "Epoch 148/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.2213\n",
      " - 0s - loss: 1.9148 - val_loss: 5.4603\n",
      "Epoch 149/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.2405\n",
      " - 0s - loss: 1.9147 - val_loss: 5.4144\n",
      "Epoch 150/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.1536\n",
      " - 0s - loss: 1.8963 - val_loss: 5.4155\n",
      "Epoch 151/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 12.4001\n",
      " - 0s - loss: 1.9012 - val_loss: 5.5504\n",
      "Epoch 152/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.1905\n",
      " - 0s - loss: 1.8792 - val_loss: 5.4374\n",
      "Epoch 153/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.1449\n",
      " - 0s - loss: 1.9159 - val_loss: 5.3772\n",
      "Epoch 154/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 12.3590\n",
      " - 0s - loss: 1.8905 - val_loss: 5.5058\n",
      "Epoch 155/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.1720\n",
      " - 0s - loss: 1.8609 - val_loss: 5.4194\n",
      "Epoch 156/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.2974\n",
      " - 0s - loss: 1.8549 - val_loss: 5.4655\n",
      "Epoch 157/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.1959\n",
      " - 0s - loss: 1.8757 - val_loss: 5.4001\n",
      "Epoch 158/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 12.2543\n",
      " - 0s - loss: 1.9215 - val_loss: 5.4946\n",
      "Epoch 159/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.4696\n",
      " - 0s - loss: 1.8955 - val_loss: 5.6553\n",
      "Epoch 160/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.1332\n",
      " - 0s - loss: 1.8763 - val_loss: 5.4881\n",
      "Epoch 161/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.2681\n",
      " - 0s - loss: 1.8828 - val_loss: 5.4919\n",
      "Epoch 162/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 12.3153\n",
      " - 0s - loss: 1.8577 - val_loss: 5.4635\n",
      "Epoch 163/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.1704\n",
      " - 0s - loss: 1.8726 - val_loss: 5.4730\n",
      "Epoch 164/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.2640\n",
      " - 0s - loss: 1.8089 - val_loss: 5.4797\n",
      "Epoch 165/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.2038\n",
      " - 0s - loss: 1.8166 - val_loss: 5.4380\n",
      "Epoch 166/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.2267\n",
      " - 0s - loss: 1.8011 - val_loss: 5.4654\n",
      "Epoch 167/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.1795\n",
      " - 0s - loss: 1.8092 - val_loss: 5.4385\n",
      "Epoch 168/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.2341\n",
      " - 0s - loss: 1.8092 - val_loss: 5.4685\n",
      "Epoch 169/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.1619\n",
      " - 0s - loss: 1.7949 - val_loss: 5.4014\n",
      "Epoch 170/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 12.1908\n",
      " - 0s - loss: 1.7982 - val_loss: 5.4438\n",
      "Epoch 171/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.2142\n",
      " - 0s - loss: 1.7881 - val_loss: 5.4339\n",
      "Epoch 172/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.2397\n",
      " - 0s - loss: 1.7916 - val_loss: 5.4378\n",
      "Epoch 173/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.1949\n",
      " - 0s - loss: 1.8163 - val_loss: 5.4423\n",
      "Epoch 174/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 12.2230\n",
      " - 0s - loss: 1.7646 - val_loss: 5.4422\n",
      "Epoch 175/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.1945\n",
      " - 0s - loss: 1.7670 - val_loss: 5.4382\n",
      "Epoch 176/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.2318\n",
      " - 0s - loss: 1.7630 - val_loss: 5.4427\n",
      "Epoch 177/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.1994\n",
      " - 0s - loss: 1.7746 - val_loss: 5.4441\n",
      "Epoch 178/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.2146\n",
      " - 0s - loss: 1.7607 - val_loss: 5.4347\n",
      "Epoch 179/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.2012\n",
      " - 0s - loss: 1.7651 - val_loss: 5.4444\n",
      "Epoch 180/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.2013\n",
      " - 0s - loss: 1.7573 - val_loss: 5.4462\n",
      "Epoch 181/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 12.2064\n",
      " - 0s - loss: 1.7586 - val_loss: 5.4426\n",
      "Epoch 182/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 12.2081\n",
      " - 0s - loss: 1.7505 - val_loss: 5.4354\n",
      "Epoch 183/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 0s 88us/sample - loss: 12.1931\n",
      " - 0s - loss: 1.7610 - val_loss: 5.4434\n",
      "Epoch 184/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.2198\n",
      " - 0s - loss: 1.7440 - val_loss: 5.4646\n",
      "Epoch 185/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.2122\n",
      " - 0s - loss: 1.7379 - val_loss: 5.4584\n",
      "Epoch 186/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 12.2033\n",
      " - 0s - loss: 1.7492 - val_loss: 5.4401\n",
      "Epoch 187/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.2181\n",
      " - 0s - loss: 1.7387 - val_loss: 5.4711\n",
      "Epoch 188/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 12.2103\n",
      " - 0s - loss: 1.7383 - val_loss: 5.4625\n",
      "Epoch 189/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.2031\n",
      " - 0s - loss: 1.7394 - val_loss: 5.4483\n",
      "Epoch 190/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.2053\n",
      " - 0s - loss: 1.7360 - val_loss: 5.4421\n",
      "Epoch 191/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.1944\n",
      " - 0s - loss: 1.7358 - val_loss: 5.4496\n",
      "Epoch 192/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.2123\n",
      " - 0s - loss: 1.7336 - val_loss: 5.4530\n",
      "Epoch 193/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.1960\n",
      " - 0s - loss: 1.7345 - val_loss: 5.4446\n",
      "Epoch 194/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.2066\n",
      " - 0s - loss: 1.7283 - val_loss: 5.4526\n",
      "Epoch 195/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 12.2124\n",
      " - 0s - loss: 1.7296 - val_loss: 5.4525\n",
      "Epoch 196/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.2027\n",
      " - 0s - loss: 1.7313 - val_loss: 5.4485\n",
      "Epoch 197/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.2002\n",
      " - 0s - loss: 1.7302 - val_loss: 5.4453\n",
      "Epoch 198/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.1974\n",
      " - 0s - loss: 1.7293 - val_loss: 5.4452\n",
      "Epoch 199/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 12.2072\n",
      " - 0s - loss: 1.7271 - val_loss: 5.4518\n",
      "Epoch 200/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.2117\n",
      " - 0s - loss: 1.7269 - val_loss: 5.4526\n",
      "Epoch 201/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.2060\n",
      " - 0s - loss: 1.7319 - val_loss: 5.4416\n",
      "Epoch 202/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.2103\n",
      " - 0s - loss: 1.7263 - val_loss: 5.4490\n",
      "Epoch 203/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.2154\n",
      " - 0s - loss: 1.7339 - val_loss: 5.4591\n",
      "Epoch 204/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.2050\n",
      " - 0s - loss: 1.7236 - val_loss: 5.4481\n",
      "Epoch 205/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.2079\n",
      " - 0s - loss: 1.7233 - val_loss: 5.4493\n",
      "Epoch 206/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.2093\n",
      " - 0s - loss: 1.7232 - val_loss: 5.4499\n",
      "Epoch 207/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.2092\n",
      " - 0s - loss: 1.7225 - val_loss: 5.4485\n",
      "Epoch 208/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.2028\n",
      " - 0s - loss: 1.7242 - val_loss: 5.4429\n",
      "Epoch 209/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.2051\n",
      " - 0s - loss: 1.7229 - val_loss: 5.4433\n",
      "Epoch 210/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 12.2094\n",
      " - 0s - loss: 1.7234 - val_loss: 5.4432\n",
      "Epoch 211/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.2065\n",
      " - 0s - loss: 1.7223 - val_loss: 5.4454\n",
      "Epoch 212/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.2080\n",
      " - 0s - loss: 1.7230 - val_loss: 5.4474\n",
      "Epoch 213/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.2096\n",
      " - 0s - loss: 1.7230 - val_loss: 5.4456\n",
      "Epoch 214/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.2107\n",
      " - 0s - loss: 1.7212 - val_loss: 5.4463\n",
      "Epoch 215/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.2097\n",
      " - 0s - loss: 1.7212 - val_loss: 5.4474\n",
      "Epoch 216/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.2077\n",
      " - 0s - loss: 1.7205 - val_loss: 5.4456\n",
      "Epoch 217/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.2119\n",
      " - 0s - loss: 1.7202 - val_loss: 5.4487\n",
      "Epoch 218/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 12.2092\n",
      " - 0s - loss: 1.7205 - val_loss: 5.4476\n",
      "Epoch 219/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.2103\n",
      " - 0s - loss: 1.7205 - val_loss: 5.4475\n",
      "Epoch 220/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.2099\n",
      " - 0s - loss: 1.7200 - val_loss: 5.4476\n",
      "Epoch 221/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 12.2108\n",
      " - 0s - loss: 1.7203 - val_loss: 5.4485\n",
      "Epoch 222/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.2078\n",
      " - 0s - loss: 1.7201 - val_loss: 5.4473\n",
      "Epoch 223/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 12.2073\n",
      " - 0s - loss: 1.7199 - val_loss: 5.4467\n",
      "Epoch 224/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 12.2080\n",
      " - 0s - loss: 1.7192 - val_loss: 5.4470\n",
      "Epoch 225/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 12.2075\n",
      " - 0s - loss: 1.7199 - val_loss: 5.4458\n",
      "Epoch 226/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.2087\n",
      " - 0s - loss: 1.7192 - val_loss: 5.4473\n",
      "Epoch 227/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.2083\n",
      " - 0s - loss: 1.7192 - val_loss: 5.4467\n",
      "Epoch 228/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.2087\n",
      " - 0s - loss: 1.7194 - val_loss: 5.4464\n",
      "Epoch 229/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 12.2091\n",
      " - 0s - loss: 1.7190 - val_loss: 5.4476\n",
      "Epoch 230/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.2087\n",
      " - 0s - loss: 1.7190 - val_loss: 5.4476\n",
      "Epoch 231/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.2092\n",
      " - 0s - loss: 1.7190 - val_loss: 5.4485\n",
      "Epoch 232/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 12.2082\n",
      " - 0s - loss: 1.7194 - val_loss: 5.4467\n",
      "Epoch 233/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 12.2092\n",
      " - 0s - loss: 1.7189 - val_loss: 5.4470\n",
      "dict_keys(['loss', 'val_loss', 'lr'])\n",
      "第 2 折\n",
      "Train on 293 samples, validate on 97 samples\n",
      "Epoch 1/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 342.8287\n",
      " - 1s - loss: 469.2621 - val_loss: 346.7467\n",
      "Epoch 2/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 102.0855\n",
      " - 0s - loss: 243.1161 - val_loss: 105.6959\n",
      "Epoch 3/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 42.4998\n",
      " - 0s - loss: 59.5946 - val_loss: 32.8184\n",
      "Epoch 4/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 28.7555\n",
      " - 0s - loss: 27.4045 - val_loss: 20.4854\n",
      "Epoch 5/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 25.1258\n",
      " - 0s - loss: 19.0709 - val_loss: 15.9899\n",
      "Epoch 6/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 23.0835\n",
      " - 0s - loss: 15.4912 - val_loss: 13.7011\n",
      "Epoch 7/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 21.8644\n",
      " - 0s - loss: 13.5387 - val_loss: 12.7437\n",
      "Epoch 8/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 20.6796\n",
      " - 0s - loss: 12.0664 - val_loss: 11.7045\n",
      "Epoch 9/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 20.0006\n",
      " - 0s - loss: 10.6863 - val_loss: 10.9106\n",
      "Epoch 10/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 19.3849\n",
      " - 0s - loss: 9.8924 - val_loss: 10.3978\n",
      "Epoch 11/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 19.2041\n",
      " - 0s - loss: 9.2792 - val_loss: 9.9820\n",
      "Epoch 12/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 18.4516\n",
      " - 0s - loss: 8.7750 - val_loss: 9.7372\n",
      "Epoch 13/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 0s 78us/sample - loss: 18.5617\n",
      " - 0s - loss: 8.5063 - val_loss: 8.8765\n",
      "Epoch 14/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 17.8094\n",
      " - 0s - loss: 8.0148 - val_loss: 9.1445\n",
      "Epoch 15/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 17.7692\n",
      " - 0s - loss: 7.6268 - val_loss: 8.6868\n",
      "Epoch 16/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 17.3515\n",
      " - 0s - loss: 7.4250 - val_loss: 8.7617\n",
      "Epoch 17/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 17.4866\n",
      " - 0s - loss: 7.1129 - val_loss: 8.4394\n",
      "Epoch 18/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 17.2690\n",
      " - 0s - loss: 6.8317 - val_loss: 8.5432\n",
      "Epoch 19/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 17.4355\n",
      " - 0s - loss: 6.6462 - val_loss: 8.0859\n",
      "Epoch 20/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.9963\n",
      " - 0s - loss: 6.3606 - val_loss: 8.0650\n",
      "Epoch 21/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.9929\n",
      " - 0s - loss: 6.3835 - val_loss: 8.5208\n",
      "Epoch 22/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 17.2801\n",
      " - 0s - loss: 6.0797 - val_loss: 7.9044\n",
      "Epoch 23/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.9100\n",
      " - 0s - loss: 6.0067 - val_loss: 7.9552\n",
      "Epoch 24/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 17.2784\n",
      " - 0s - loss: 5.8465 - val_loss: 8.1960\n",
      "Epoch 25/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.5254\n",
      " - 0s - loss: 5.7566 - val_loss: 8.4485\n",
      "Epoch 26/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.6530\n",
      " - 0s - loss: 5.6652 - val_loss: 8.0509\n",
      "Epoch 27/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.5002\n",
      " - 0s - loss: 5.5154 - val_loss: 8.3307\n",
      "Epoch 28/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.8261\n",
      " - 0s - loss: 5.4114 - val_loss: 8.2055\n",
      "Epoch 29/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 17.4806\n",
      " - 0s - loss: 5.2685 - val_loss: 7.8646\n",
      "Epoch 30/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.5656\n",
      " - 0s - loss: 5.3066 - val_loss: 7.8479\n",
      "Epoch 31/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 17.0358\n",
      " - 0s - loss: 5.2238 - val_loss: 7.9772\n",
      "Epoch 32/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.4233\n",
      " - 0s - loss: 5.1110 - val_loss: 8.5473\n",
      "Epoch 33/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 16.9528\n",
      " - 0s - loss: 4.9602 - val_loss: 8.3353\n",
      "Epoch 34/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.7096\n",
      " - 0s - loss: 4.9788 - val_loss: 8.3250\n",
      "Epoch 35/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1934\n",
      " - 0s - loss: 4.8683 - val_loss: 8.5249\n",
      "Epoch 36/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.8684\n",
      " - 0s - loss: 4.8245 - val_loss: 8.3151\n",
      "Epoch 37/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.3976\n",
      " - 0s - loss: 4.7553 - val_loss: 8.1702\n",
      "Epoch 38/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 17.0164\n",
      " - 0s - loss: 4.6366 - val_loss: 8.4332\n",
      "Epoch 39/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.5099\n",
      " - 0s - loss: 4.5540 - val_loss: 8.0803\n",
      "Epoch 40/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.8693\n",
      " - 0s - loss: 4.6892 - val_loss: 7.9828\n",
      "Epoch 41/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 16.2467\n",
      " - 0s - loss: 4.3848 - val_loss: 8.2853\n",
      "Epoch 42/500\n",
      "102/102 [==============================] - 0s 79us/sample - loss: 16.3711\n",
      " - 0s - loss: 4.4038 - val_loss: 8.1739\n",
      "Epoch 43/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.2500\n",
      " - 0s - loss: 4.3045 - val_loss: 8.2814\n",
      "Epoch 44/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.3814\n",
      " - 0s - loss: 4.2952 - val_loss: 8.3055\n",
      "Epoch 45/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.3890\n",
      " - 0s - loss: 4.2719 - val_loss: 8.2560\n",
      "Epoch 46/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 16.3761\n",
      " - 0s - loss: 4.2587 - val_loss: 8.3832\n",
      "Epoch 47/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.4509\n",
      " - 0s - loss: 4.2116 - val_loss: 8.5183\n",
      "Epoch 48/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.6020\n",
      " - 0s - loss: 4.2306 - val_loss: 8.5327\n",
      "Epoch 49/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.0834\n",
      " - 0s - loss: 4.1559 - val_loss: 8.4497\n",
      "Epoch 50/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.3203\n",
      " - 0s - loss: 4.2027 - val_loss: 8.2120\n",
      "Epoch 51/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 16.1181\n",
      " - 0s - loss: 4.1061 - val_loss: 8.4275\n",
      "Epoch 52/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1500\n",
      " - 0s - loss: 4.0922 - val_loss: 8.4500\n",
      "Epoch 53/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1620\n",
      " - 0s - loss: 4.0565 - val_loss: 8.3417\n",
      "Epoch 54/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1787\n",
      " - 0s - loss: 4.0289 - val_loss: 8.3330\n",
      "Epoch 55/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.2124\n",
      " - 0s - loss: 4.0715 - val_loss: 8.3664\n",
      "Epoch 56/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1454\n",
      " - 0s - loss: 4.0225 - val_loss: 8.3817\n",
      "Epoch 57/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.0698\n",
      " - 0s - loss: 4.0038 - val_loss: 8.3649\n",
      "Epoch 58/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1834\n",
      " - 0s - loss: 3.9989 - val_loss: 8.4996\n",
      "Epoch 59/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1020\n",
      " - 0s - loss: 3.9754 - val_loss: 8.4289\n",
      "Epoch 60/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.0923\n",
      " - 0s - loss: 3.9901 - val_loss: 8.3819\n",
      "Epoch 61/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.0679\n",
      " - 0s - loss: 3.9208 - val_loss: 8.4938\n",
      "Epoch 62/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1596\n",
      " - 0s - loss: 3.9188 - val_loss: 8.3957\n",
      "Epoch 63/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1248\n",
      " - 0s - loss: 3.9251 - val_loss: 8.4159\n",
      "Epoch 64/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1214\n",
      " - 0s - loss: 3.9097 - val_loss: 8.4319\n",
      "Epoch 65/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1191\n",
      " - 0s - loss: 3.9204 - val_loss: 8.4469\n",
      "Epoch 66/500\n",
      "102/102 [==============================] - 0s 76us/sample - loss: 16.1106\n",
      " - 0s - loss: 3.9289 - val_loss: 8.3952\n",
      "Epoch 67/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1029\n",
      " - 0s - loss: 3.9235 - val_loss: 8.4207\n",
      "Epoch 68/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.0919\n",
      " - 0s - loss: 3.9052 - val_loss: 8.4480\n",
      "Epoch 69/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 16.0765\n",
      " - 0s - loss: 3.9012 - val_loss: 8.4790\n",
      "Epoch 70/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1162\n",
      " - 0s - loss: 3.8879 - val_loss: 8.4333\n",
      "Epoch 71/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 16.1140\n",
      " - 0s - loss: 3.8701 - val_loss: 8.4953\n",
      "Epoch 72/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.0653\n",
      " - 0s - loss: 3.8753 - val_loss: 8.4644\n",
      "Epoch 73/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1321\n",
      " - 0s - loss: 3.8572 - val_loss: 8.4165\n",
      "Epoch 74/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.0804\n",
      " - 0s - loss: 3.8645 - val_loss: 8.4853\n",
      "Epoch 75/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.0937\n",
      " - 0s - loss: 3.8489 - val_loss: 8.4596\n",
      "Epoch 76/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 16.0703\n",
      " - 0s - loss: 3.8463 - val_loss: 8.4538\n",
      "Epoch 77/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1032\n",
      " - 0s - loss: 3.8423 - val_loss: 8.4668\n",
      "Epoch 78/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1166\n",
      " - 0s - loss: 3.8581 - val_loss: 8.4723\n",
      "Epoch 79/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 16.0869\n",
      " - 0s - loss: 3.8390 - val_loss: 8.4904\n",
      "Epoch 80/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 16.1352\n",
      " - 0s - loss: 3.8384 - val_loss: 8.4977\n",
      "Epoch 81/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1068\n",
      " - 0s - loss: 3.8348 - val_loss: 8.5039\n",
      "Epoch 82/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1049\n",
      " - 0s - loss: 3.8269 - val_loss: 8.5117\n",
      "Epoch 83/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1103\n",
      " - 0s - loss: 3.8346 - val_loss: 8.4931\n",
      "Epoch 84/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 16.1031\n",
      " - 0s - loss: 3.8197 - val_loss: 8.5141\n",
      "Epoch 85/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1010\n",
      " - 0s - loss: 3.8248 - val_loss: 8.5287\n",
      "Epoch 86/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1077\n",
      " - 0s - loss: 3.8234 - val_loss: 8.4982\n",
      "Epoch 87/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1140\n",
      " - 0s - loss: 3.8218 - val_loss: 8.5170\n",
      "Epoch 88/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1011\n",
      " - 0s - loss: 3.8197 - val_loss: 8.5172\n",
      "Epoch 89/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 16.1192\n",
      " - 0s - loss: 3.8138 - val_loss: 8.4846\n",
      "Epoch 90/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1143\n",
      " - 0s - loss: 3.8206 - val_loss: 8.4988\n",
      "Epoch 91/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1134\n",
      " - 0s - loss: 3.8087 - val_loss: 8.5036\n",
      "Epoch 92/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1128\n",
      " - 0s - loss: 3.8048 - val_loss: 8.5063\n",
      "Epoch 93/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1079\n",
      " - 0s - loss: 3.8050 - val_loss: 8.5159\n",
      "Epoch 94/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1155\n",
      " - 0s - loss: 3.8045 - val_loss: 8.5029\n",
      "Epoch 95/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1216\n",
      " - 0s - loss: 3.8051 - val_loss: 8.5018\n",
      "Epoch 96/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1008\n",
      " - 0s - loss: 3.8028 - val_loss: 8.5129\n",
      "Epoch 97/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1076\n",
      " - 0s - loss: 3.8020 - val_loss: 8.5158\n",
      "Epoch 98/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1035\n",
      " - 0s - loss: 3.8015 - val_loss: 8.5154\n",
      "Epoch 99/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1134\n",
      " - 0s - loss: 3.8051 - val_loss: 8.5182\n",
      "Epoch 100/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1113\n",
      " - 0s - loss: 3.8014 - val_loss: 8.5100\n",
      "Epoch 101/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1059\n",
      " - 0s - loss: 3.7990 - val_loss: 8.5147\n",
      "Epoch 102/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1059\n",
      " - 0s - loss: 3.7987 - val_loss: 8.5125\n",
      "Epoch 103/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 16.1081\n",
      " - 0s - loss: 3.7971 - val_loss: 8.5127\n",
      "Epoch 104/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 16.1095\n",
      " - 0s - loss: 3.7964 - val_loss: 8.5179\n",
      "Epoch 105/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1021\n",
      " - 0s - loss: 3.7961 - val_loss: 8.5171\n",
      "Epoch 106/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1049\n",
      " - 0s - loss: 3.7955 - val_loss: 8.5097\n",
      "Epoch 107/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1070\n",
      " - 0s - loss: 3.7971 - val_loss: 8.5097\n",
      "Epoch 108/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.0974\n",
      " - 0s - loss: 3.7971 - val_loss: 8.5207\n",
      "Epoch 109/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1067\n",
      " - 0s - loss: 3.7937 - val_loss: 8.5143\n",
      "Epoch 110/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 16.1021\n",
      " - 0s - loss: 3.7950 - val_loss: 8.5118\n",
      "dict_keys(['loss', 'val_loss', 'lr'])\n",
      "第 3 折\n",
      "Train on 293 samples, validate on 97 samples\n",
      "Epoch 1/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 386.8812\n",
      " - 1s - loss: 468.6201 - val_loss: 453.3661\n",
      "Epoch 2/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 172.0923\n",
      " - 0s - loss: 299.6877 - val_loss: 228.7983\n",
      "Epoch 3/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 55.4518\n",
      " - 0s - loss: 99.1167 - val_loss: 63.6951\n",
      "Epoch 4/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 34.9309\n",
      " - 0s - loss: 37.2808 - val_loss: 33.1532\n",
      "Epoch 5/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 25.6025\n",
      " - 0s - loss: 20.8958 - val_loss: 18.7998\n",
      "Epoch 6/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 21.8744\n",
      " - 0s - loss: 15.0858 - val_loss: 14.5014\n",
      "Epoch 7/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 20.7393\n",
      " - 0s - loss: 12.3653 - val_loss: 12.0926\n",
      "Epoch 8/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 19.8498\n",
      " - 0s - loss: 11.0179 - val_loss: 10.6207\n",
      "Epoch 9/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 18.5639\n",
      " - 0s - loss: 9.9987 - val_loss: 9.7012\n",
      "Epoch 10/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 18.5590\n",
      " - 0s - loss: 9.1809 - val_loss: 9.5236\n",
      "Epoch 11/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 18.3512\n",
      " - 0s - loss: 8.6324 - val_loss: 9.5162\n",
      "Epoch 12/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 17.2206\n",
      " - 0s - loss: 8.2258 - val_loss: 8.3074\n",
      "Epoch 13/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 17.9348\n",
      " - 0s - loss: 7.8371 - val_loss: 8.6062\n",
      "Epoch 14/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.8564\n",
      " - 0s - loss: 7.5449 - val_loss: 8.1347\n",
      "Epoch 15/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 17.1031\n",
      " - 0s - loss: 7.2989 - val_loss: 8.1631\n",
      "Epoch 16/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.8341\n",
      " - 0s - loss: 6.9628 - val_loss: 7.8759\n",
      "Epoch 17/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.9320\n",
      " - 0s - loss: 6.7134 - val_loss: 7.7973\n",
      "Epoch 18/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 17.5787\n",
      " - 0s - loss: 6.5878 - val_loss: 7.9167\n",
      "Epoch 19/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.6503\n",
      " - 0s - loss: 6.3438 - val_loss: 7.3718\n",
      "Epoch 20/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 17.1333\n",
      " - 0s - loss: 6.1823 - val_loss: 7.5979\n",
      "Epoch 21/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 17.0586\n",
      " - 0s - loss: 6.1088 - val_loss: 7.5972\n",
      "Epoch 22/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.5317\n",
      " - 0s - loss: 5.9020 - val_loss: 7.3597\n",
      "Epoch 23/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.4359\n",
      " - 0s - loss: 5.9027 - val_loss: 7.1589\n",
      "Epoch 24/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 17.2520\n",
      " - 0s - loss: 5.8541 - val_loss: 7.6974\n",
      "Epoch 25/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 17.1793\n",
      " - 0s - loss: 5.6590 - val_loss: 7.8251\n",
      "Epoch 26/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.9054\n",
      " - 0s - loss: 5.5861 - val_loss: 7.2584\n",
      "Epoch 27/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.9835\n",
      " - 0s - loss: 5.5158 - val_loss: 7.2346\n",
      "Epoch 28/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 17.0110\n",
      " - 0s - loss: 5.5843 - val_loss: 7.2940\n",
      "Epoch 29/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.6750\n",
      " - 0s - loss: 5.5267 - val_loss: 7.0562\n",
      "Epoch 30/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 17.3848\n",
      " - 0s - loss: 5.4194 - val_loss: 7.3742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 17.1262\n",
      " - 0s - loss: 5.3040 - val_loss: 7.1889\n",
      "Epoch 32/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.5354\n",
      " - 0s - loss: 5.2719 - val_loss: 6.9385\n",
      "Epoch 33/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 17.5715\n",
      " - 0s - loss: 5.1458 - val_loss: 8.4525\n",
      "Epoch 34/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.7717\n",
      " - 0s - loss: 5.5990 - val_loss: 7.2020\n",
      "Epoch 35/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 17.1042\n",
      " - 0s - loss: 5.1172 - val_loss: 7.1012\n",
      "Epoch 36/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.6552\n",
      " - 0s - loss: 4.9467 - val_loss: 7.0199\n",
      "Epoch 37/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 16.9954\n",
      " - 0s - loss: 5.1279 - val_loss: 7.7204\n",
      "Epoch 38/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.5515\n",
      " - 0s - loss: 5.0963 - val_loss: 7.1036\n",
      "Epoch 39/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.8497\n",
      " - 0s - loss: 4.8974 - val_loss: 7.0580\n",
      "Epoch 40/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.9632\n",
      " - 0s - loss: 4.8641 - val_loss: 6.9676\n",
      "Epoch 41/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.9561\n",
      " - 0s - loss: 4.8414 - val_loss: 6.9201\n",
      "Epoch 42/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 16.7286\n",
      " - 0s - loss: 4.7192 - val_loss: 6.8334\n",
      "Epoch 43/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.3651\n",
      " - 0s - loss: 4.6712 - val_loss: 6.5218\n",
      "Epoch 44/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.6003\n",
      " - 0s - loss: 4.8498 - val_loss: 6.6157\n",
      "Epoch 45/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.3416\n",
      " - 0s - loss: 4.6899 - val_loss: 6.4543\n",
      "Epoch 46/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 17.1904\n",
      " - 0s - loss: 4.5268 - val_loss: 7.1570\n",
      "Epoch 47/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 18.4225\n",
      " - 0s - loss: 4.5766 - val_loss: 8.2067\n",
      "Epoch 48/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 16.6618\n",
      " - 0s - loss: 4.9538 - val_loss: 7.0309\n",
      "Epoch 49/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 17.0506\n",
      " - 0s - loss: 4.4808 - val_loss: 6.6923\n",
      "Epoch 50/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.9465\n",
      " - 0s - loss: 4.6022 - val_loss: 6.7062\n",
      "Epoch 51/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.2223\n",
      " - 0s - loss: 4.5099 - val_loss: 6.4021\n",
      "Epoch 52/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 17.1822\n",
      " - 0s - loss: 4.4672 - val_loss: 6.8778\n",
      "Epoch 53/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 17.2736\n",
      " - 0s - loss: 4.5505 - val_loss: 7.2544\n",
      "Epoch 54/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.9988\n",
      " - 0s - loss: 4.3050 - val_loss: 6.9835\n",
      "Epoch 55/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 17.3025\n",
      " - 0s - loss: 4.4008 - val_loss: 6.8112\n",
      "Epoch 56/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 17.1739\n",
      " - 0s - loss: 4.2122 - val_loss: 7.0531\n",
      "Epoch 57/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.9036\n",
      " - 0s - loss: 4.1967 - val_loss: 6.5531\n",
      "Epoch 58/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.7364\n",
      " - 0s - loss: 4.2961 - val_loss: 7.0701\n",
      "Epoch 59/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 17.2338\n",
      " - 0s - loss: 4.6188 - val_loss: 6.9992\n",
      "Epoch 60/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.6437\n",
      " - 0s - loss: 4.4137 - val_loss: 6.6448\n",
      "Epoch 61/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 16.1331\n",
      " - 0s - loss: 4.2754 - val_loss: 6.3106\n",
      "Epoch 62/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 16.5682\n",
      " - 0s - loss: 4.0663 - val_loss: 6.2860\n",
      "Epoch 63/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 16.9231\n",
      " - 0s - loss: 4.0611 - val_loss: 6.7300\n",
      "Epoch 64/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.2239\n",
      " - 0s - loss: 3.9206 - val_loss: 6.2805\n",
      "Epoch 65/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 16.6639\n",
      " - 0s - loss: 4.0605 - val_loss: 6.3553\n",
      "Epoch 66/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 16.7109\n",
      " - 0s - loss: 3.9464 - val_loss: 6.4842\n",
      "Epoch 67/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 15.9940\n",
      " - 0s - loss: 3.9203 - val_loss: 5.9228\n",
      "Epoch 68/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 15.9342\n",
      " - 0s - loss: 3.9192 - val_loss: 6.0940\n",
      "Epoch 69/500\n",
      "102/102 [==============================] - 0s 108us/sample - loss: 17.7901\n",
      " - 0s - loss: 3.8640 - val_loss: 7.0658\n",
      "Epoch 70/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.3536\n",
      " - 0s - loss: 3.8972 - val_loss: 6.2829\n",
      "Epoch 71/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.8263\n",
      " - 0s - loss: 3.8555 - val_loss: 6.2777\n",
      "Epoch 72/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.9584\n",
      " - 0s - loss: 3.7983 - val_loss: 6.4140\n",
      "Epoch 73/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.9965\n",
      " - 0s - loss: 3.7674 - val_loss: 6.4213\n",
      "Epoch 74/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.4868\n",
      " - 0s - loss: 3.7558 - val_loss: 6.3667\n",
      "Epoch 75/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 16.5813\n",
      " - 0s - loss: 3.8714 - val_loss: 6.1323\n",
      "Epoch 76/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 17.1507\n",
      " - 0s - loss: 3.7192 - val_loss: 6.6342\n",
      "Epoch 77/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.0158\n",
      " - 0s - loss: 3.6745 - val_loss: 6.0093\n",
      "Epoch 78/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.5325\n",
      " - 0s - loss: 3.4684 - val_loss: 6.2675\n",
      "Epoch 79/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.5224\n",
      " - 0s - loss: 3.4737 - val_loss: 6.1481\n",
      "Epoch 80/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 16.3954\n",
      " - 0s - loss: 3.4499 - val_loss: 6.2124\n",
      "Epoch 81/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.2260\n",
      " - 0s - loss: 3.4140 - val_loss: 5.8849\n",
      "Epoch 82/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.3067\n",
      " - 0s - loss: 3.4690 - val_loss: 6.0104\n",
      "Epoch 83/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1727\n",
      " - 0s - loss: 3.4619 - val_loss: 6.0156\n",
      "Epoch 84/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.2256\n",
      " - 0s - loss: 3.3956 - val_loss: 5.9190\n",
      "Epoch 85/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.4157\n",
      " - 0s - loss: 3.2854 - val_loss: 6.2347\n",
      "Epoch 86/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.2973\n",
      " - 0s - loss: 3.3979 - val_loss: 6.0747\n",
      "Epoch 87/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.2234\n",
      " - 0s - loss: 3.3736 - val_loss: 5.9869\n",
      "Epoch 88/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1790\n",
      " - 0s - loss: 3.3222 - val_loss: 5.7957\n",
      "Epoch 89/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.3983\n",
      " - 0s - loss: 3.3803 - val_loss: 5.9637\n",
      "Epoch 90/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.0692\n",
      " - 0s - loss: 3.3514 - val_loss: 5.8658\n",
      "Epoch 91/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.6574\n",
      " - 0s - loss: 3.4246 - val_loss: 6.0956\n",
      "Epoch 92/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.3169\n",
      " - 0s - loss: 3.2665 - val_loss: 6.1973\n",
      "Epoch 93/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.2548\n",
      " - 0s - loss: 3.2904 - val_loss: 5.9113\n",
      "Epoch 94/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.2796\n",
      " - 0s - loss: 3.2257 - val_loss: 6.0376\n",
      "Epoch 95/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 0s 78us/sample - loss: 16.4947\n",
      " - 0s - loss: 3.2029 - val_loss: 6.1815\n",
      "Epoch 96/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 15.9330\n",
      " - 0s - loss: 3.2816 - val_loss: 5.8714\n",
      "Epoch 97/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.4645\n",
      " - 0s - loss: 3.2061 - val_loss: 6.0606\n",
      "Epoch 98/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.2673\n",
      " - 0s - loss: 3.2046 - val_loss: 5.8812\n",
      "Epoch 99/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.2775\n",
      " - 0s - loss: 3.1263 - val_loss: 5.9126\n",
      "Epoch 100/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1813\n",
      " - 0s - loss: 3.1322 - val_loss: 6.0382\n",
      "Epoch 101/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1279\n",
      " - 0s - loss: 3.1213 - val_loss: 5.9052\n",
      "Epoch 102/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1801\n",
      " - 0s - loss: 3.0894 - val_loss: 5.9724\n",
      "Epoch 103/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1926\n",
      " - 0s - loss: 3.1315 - val_loss: 5.9453\n",
      "Epoch 104/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1652\n",
      " - 0s - loss: 3.1158 - val_loss: 5.8605\n",
      "Epoch 105/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.2625\n",
      " - 0s - loss: 3.0627 - val_loss: 5.9970\n",
      "Epoch 106/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1214\n",
      " - 0s - loss: 3.0785 - val_loss: 5.9646\n",
      "Epoch 107/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.3177\n",
      " - 0s - loss: 3.0795 - val_loss: 5.9715\n",
      "Epoch 108/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1352\n",
      " - 0s - loss: 3.0672 - val_loss: 5.9385\n",
      "Epoch 109/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.0806\n",
      " - 0s - loss: 3.0150 - val_loss: 5.8271\n",
      "Epoch 110/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1366\n",
      " - 0s - loss: 3.0076 - val_loss: 5.8893\n",
      "Epoch 111/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1220\n",
      " - 0s - loss: 3.0025 - val_loss: 5.8593\n",
      "Epoch 112/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.0747\n",
      " - 0s - loss: 3.0000 - val_loss: 5.8555\n",
      "Epoch 113/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.0710\n",
      " - 0s - loss: 3.0099 - val_loss: 5.8375\n",
      "Epoch 114/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1351\n",
      " - 0s - loss: 2.9799 - val_loss: 5.8835\n",
      "Epoch 115/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1470\n",
      " - 0s - loss: 2.9985 - val_loss: 5.9043\n",
      "Epoch 116/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 16.1301\n",
      " - 0s - loss: 3.0344 - val_loss: 5.9174\n",
      "Epoch 117/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.0845\n",
      " - 0s - loss: 2.9781 - val_loss: 5.8441\n",
      "Epoch 118/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1672\n",
      " - 0s - loss: 2.9893 - val_loss: 5.9381\n",
      "Epoch 119/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1571\n",
      " - 0s - loss: 2.9557 - val_loss: 5.9220\n",
      "Epoch 120/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1037\n",
      " - 0s - loss: 2.9553 - val_loss: 5.8658\n",
      "Epoch 121/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.0952\n",
      " - 0s - loss: 2.9528 - val_loss: 5.8755\n",
      "Epoch 122/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.0888\n",
      " - 0s - loss: 2.9822 - val_loss: 5.8510\n",
      "Epoch 123/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1129\n",
      " - 0s - loss: 2.9445 - val_loss: 5.8817\n",
      "Epoch 124/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1157\n",
      " - 0s - loss: 2.9423 - val_loss: 5.8740\n",
      "Epoch 125/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1140\n",
      " - 0s - loss: 2.9418 - val_loss: 5.8757\n",
      "Epoch 126/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.0988\n",
      " - 0s - loss: 2.9487 - val_loss: 5.8522\n",
      "Epoch 127/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.0745\n",
      " - 0s - loss: 2.9476 - val_loss: 5.8637\n",
      "Epoch 128/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1716\n",
      " - 0s - loss: 2.9398 - val_loss: 5.9207\n",
      "Epoch 129/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.0982\n",
      " - 0s - loss: 2.9258 - val_loss: 5.8795\n",
      "Epoch 130/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1046\n",
      " - 0s - loss: 2.9186 - val_loss: 5.8613\n",
      "Epoch 131/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1159\n",
      " - 0s - loss: 2.9254 - val_loss: 5.8540\n",
      "Epoch 132/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1069\n",
      " - 0s - loss: 2.9194 - val_loss: 5.8587\n",
      "Epoch 133/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1125\n",
      " - 0s - loss: 2.9216 - val_loss: 5.8723\n",
      "Epoch 134/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1058\n",
      " - 0s - loss: 2.9250 - val_loss: 5.8660\n",
      "Epoch 135/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1269\n",
      " - 0s - loss: 2.9196 - val_loss: 5.8650\n",
      "Epoch 136/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 16.1060\n",
      " - 0s - loss: 2.9167 - val_loss: 5.8640\n",
      "Epoch 137/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1016\n",
      " - 0s - loss: 2.9184 - val_loss: 5.8516\n",
      "Epoch 138/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1279\n",
      " - 0s - loss: 2.9217 - val_loss: 5.8842\n",
      "Epoch 139/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1079\n",
      " - 0s - loss: 2.9097 - val_loss: 5.8620\n",
      "Epoch 140/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.0978\n",
      " - 0s - loss: 2.9122 - val_loss: 5.8506\n",
      "Epoch 141/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.0987\n",
      " - 0s - loss: 2.9106 - val_loss: 5.8587\n",
      "Epoch 142/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.0927\n",
      " - 0s - loss: 2.9094 - val_loss: 5.8521\n",
      "Epoch 143/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1069\n",
      " - 0s - loss: 2.9088 - val_loss: 5.8599\n",
      "Epoch 144/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1007\n",
      " - 0s - loss: 2.9082 - val_loss: 5.8546\n",
      "Epoch 145/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1094\n",
      " - 0s - loss: 2.9105 - val_loss: 5.8594\n",
      "Epoch 146/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1068\n",
      " - 0s - loss: 2.9069 - val_loss: 5.8599\n",
      "Epoch 147/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.0975\n",
      " - 0s - loss: 2.9057 - val_loss: 5.8556\n",
      "Epoch 148/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 16.1041\n",
      " - 0s - loss: 2.9089 - val_loss: 5.8607\n",
      "Epoch 149/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1031\n",
      " - 0s - loss: 2.9035 - val_loss: 5.8609\n",
      "Epoch 150/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1059\n",
      " - 0s - loss: 2.9039 - val_loss: 5.8621\n",
      "Epoch 151/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1055\n",
      " - 0s - loss: 2.9037 - val_loss: 5.8611\n",
      "Epoch 152/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1055\n",
      " - 0s - loss: 2.9046 - val_loss: 5.8653\n",
      "Epoch 153/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.0927\n",
      " - 0s - loss: 2.9026 - val_loss: 5.8531\n",
      "Epoch 154/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1013\n",
      " - 0s - loss: 2.9031 - val_loss: 5.8540\n",
      "Epoch 155/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1014\n",
      " - 0s - loss: 2.9026 - val_loss: 5.8544\n",
      "Epoch 156/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1034\n",
      " - 0s - loss: 2.9007 - val_loss: 5.8598\n",
      "Epoch 157/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1040\n",
      " - 0s - loss: 2.9014 - val_loss: 5.8573\n",
      "Epoch 158/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1033\n",
      " - 0s - loss: 2.9035 - val_loss: 5.8533\n",
      "Epoch 159/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 0s 98us/sample - loss: 16.1048\n",
      " - 0s - loss: 2.9000 - val_loss: 5.8563\n",
      "Epoch 160/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 16.1037\n",
      " - 0s - loss: 2.8996 - val_loss: 5.8560\n",
      "Epoch 161/500\n",
      "102/102 [==============================] - 0s 108us/sample - loss: 16.1012\n",
      " - 0s - loss: 2.8994 - val_loss: 5.8553\n",
      "Epoch 162/500\n",
      "102/102 [==============================] - 0s 108us/sample - loss: 16.1046\n",
      " - 0s - loss: 2.8993 - val_loss: 5.8565\n",
      "Epoch 163/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 16.1020\n",
      " - 0s - loss: 2.8992 - val_loss: 5.8558\n",
      "Epoch 164/500\n",
      "102/102 [==============================] - 0s 108us/sample - loss: 16.1014\n",
      " - 0s - loss: 2.8994 - val_loss: 5.8538\n",
      "Epoch 165/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1001\n",
      " - 0s - loss: 2.8989 - val_loss: 5.8531\n",
      "Epoch 166/500\n",
      "102/102 [==============================] - 0s 108us/sample - loss: 16.1020\n",
      " - 0s - loss: 2.8990 - val_loss: 5.8566\n",
      "Epoch 167/500\n",
      "102/102 [==============================] - 0s 108us/sample - loss: 16.1048\n",
      " - 0s - loss: 2.8989 - val_loss: 5.8577\n",
      "Epoch 168/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1028\n",
      " - 0s - loss: 2.9005 - val_loss: 5.8579\n",
      "dict_keys(['loss', 'val_loss', 'lr'])\n",
      "第 4 折\n",
      "Train on 293 samples, validate on 97 samples\n",
      "Epoch 1/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 381.1493\n",
      " - 1s - loss: 480.6755 - val_loss: 392.4747\n",
      "Epoch 2/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 149.2218\n",
      " - 0s - loss: 290.0865 - val_loss: 141.4746\n",
      "Epoch 3/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 49.2618\n",
      " - 0s - loss: 87.4912 - val_loss: 37.7803\n",
      "Epoch 4/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 31.0152\n",
      " - 0s - loss: 35.1279 - val_loss: 19.6864\n",
      "Epoch 5/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 25.8468\n",
      " - 0s - loss: 20.4536 - val_loss: 16.4235\n",
      "Epoch 6/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 24.2015\n",
      " - 0s - loss: 15.9048 - val_loss: 15.8090\n",
      "Epoch 7/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 22.6397\n",
      " - 0s - loss: 13.5028 - val_loss: 14.9109\n",
      "Epoch 8/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 21.9799\n",
      " - 0s - loss: 11.9431 - val_loss: 14.3595\n",
      "Epoch 9/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 20.9198\n",
      " - 0s - loss: 10.8406 - val_loss: 13.4404\n",
      "Epoch 10/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 20.2943\n",
      " - 0s - loss: 9.8688 - val_loss: 13.3558\n",
      "Epoch 11/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 19.7660\n",
      " - 0s - loss: 9.1774 - val_loss: 12.5623\n",
      "Epoch 12/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 19.5279\n",
      " - 0s - loss: 8.8176 - val_loss: 12.5563\n",
      "Epoch 13/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 18.6595\n",
      " - 0s - loss: 8.2197 - val_loss: 12.0677\n",
      "Epoch 14/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 18.3987\n",
      " - 0s - loss: 7.8371 - val_loss: 11.6703\n",
      "Epoch 15/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 18.3582\n",
      " - 0s - loss: 7.5156 - val_loss: 11.4051\n",
      "Epoch 16/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 18.1867\n",
      " - 0s - loss: 7.3758 - val_loss: 11.9367\n",
      "Epoch 17/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 17.8186\n",
      " - 0s - loss: 7.1561 - val_loss: 11.1513\n",
      "Epoch 18/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 17.3079\n",
      " - 0s - loss: 6.7276 - val_loss: 10.6739\n",
      "Epoch 19/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 17.1292\n",
      " - 0s - loss: 6.4549 - val_loss: 10.7782\n",
      "Epoch 20/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.9122\n",
      " - 0s - loss: 6.3037 - val_loss: 10.3408\n",
      "Epoch 21/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.8753\n",
      " - 0s - loss: 6.1044 - val_loss: 10.8338\n",
      "Epoch 22/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.6179\n",
      " - 0s - loss: 5.9637 - val_loss: 10.3002\n",
      "Epoch 23/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.8722\n",
      " - 0s - loss: 5.8563 - val_loss: 10.0697\n",
      "Epoch 24/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.5127\n",
      " - 0s - loss: 5.7070 - val_loss: 10.2801\n",
      "Epoch 25/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.3123\n",
      " - 0s - loss: 5.6712 - val_loss: 9.5879\n",
      "Epoch 26/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.3144\n",
      " - 0s - loss: 5.5067 - val_loss: 9.9003\n",
      "Epoch 27/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.4828\n",
      " - 0s - loss: 5.4483 - val_loss: 9.3646\n",
      "Epoch 28/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 16.1437\n",
      " - 0s - loss: 5.3095 - val_loss: 9.6358\n",
      "Epoch 29/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1775\n",
      " - 0s - loss: 5.1600 - val_loss: 9.2783\n",
      "Epoch 30/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.1840\n",
      " - 0s - loss: 5.1697 - val_loss: 9.1651\n",
      "Epoch 31/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.3191\n",
      " - 0s - loss: 4.9903 - val_loss: 9.4704\n",
      "Epoch 32/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 15.9502\n",
      " - 0s - loss: 4.9512 - val_loss: 9.1645\n",
      "Epoch 33/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 15.8375\n",
      " - 0s - loss: 4.9588 - val_loss: 9.1666\n",
      "Epoch 34/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 15.9913\n",
      " - 0s - loss: 4.8134 - val_loss: 9.2745\n",
      "Epoch 35/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 15.9142\n",
      " - 0s - loss: 4.7263 - val_loss: 8.9806\n",
      "Epoch 36/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 15.9375\n",
      " - 0s - loss: 4.7758 - val_loss: 9.0422\n",
      "Epoch 37/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 15.7773\n",
      " - 0s - loss: 4.7616 - val_loss: 9.3690\n",
      "Epoch 38/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 15.7452\n",
      " - 0s - loss: 4.7628 - val_loss: 9.2462\n",
      "Epoch 39/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 15.9931\n",
      " - 0s - loss: 4.4908 - val_loss: 8.8179\n",
      "Epoch 40/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 16.0279\n",
      " - 0s - loss: 4.2911 - val_loss: 9.1036\n",
      "Epoch 41/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 15.2963\n",
      " - 0s - loss: 4.3287 - val_loss: 8.8359\n",
      "Epoch 42/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 15.7824\n",
      " - 0s - loss: 4.5006 - val_loss: 10.0840\n",
      "Epoch 43/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 15.3927\n",
      " - 0s - loss: 4.2783 - val_loss: 8.9661\n",
      "Epoch 44/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 15.5356\n",
      " - 0s - loss: 4.1471 - val_loss: 9.2342\n",
      "Epoch 45/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 15.3200\n",
      " - 0s - loss: 4.0478 - val_loss: 8.7935\n",
      "Epoch 46/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 15.2657\n",
      " - 0s - loss: 4.1444 - val_loss: 9.0426\n",
      "Epoch 47/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 15.1636\n",
      " - 0s - loss: 4.1541 - val_loss: 9.3819\n",
      "Epoch 48/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 15.2363\n",
      " - 0s - loss: 4.0781 - val_loss: 8.6986\n",
      "Epoch 49/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 15.0908\n",
      " - 0s - loss: 3.9366 - val_loss: 8.6863\n",
      "Epoch 50/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 15.8068\n",
      " - 0s - loss: 3.9022 - val_loss: 8.3318\n",
      "Epoch 51/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 15.7225\n",
      " - 0s - loss: 3.8072 - val_loss: 9.5336\n",
      "Epoch 52/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 15.2871\n",
      " - 0s - loss: 3.9811 - val_loss: 9.0643\n",
      "Epoch 53/500\n",
      "102/102 [==============================] - 0s 68us/sample - loss: 15.1975\n",
      " - 0s - loss: 3.8540 - val_loss: 9.1304\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 0s 88us/sample - loss: 15.0229\n",
      " - 0s - loss: 3.7153 - val_loss: 8.9493\n",
      "Epoch 55/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 14.9109\n",
      " - 0s - loss: 3.6079 - val_loss: 8.5741\n",
      "Epoch 56/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 15.1955\n",
      " - 0s - loss: 3.6789 - val_loss: 8.8768\n",
      "Epoch 57/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 15.4914\n",
      " - 0s - loss: 3.6495 - val_loss: 8.5474\n",
      "Epoch 58/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 14.9068\n",
      " - 0s - loss: 3.6485 - val_loss: 8.7299\n",
      "Epoch 59/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 15.1804\n",
      " - 0s - loss: 3.5418 - val_loss: 8.6001\n",
      "Epoch 60/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 15.1387\n",
      " - 0s - loss: 3.4581 - val_loss: 8.5181\n",
      "Epoch 61/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.9753\n",
      " - 0s - loss: 3.3130 - val_loss: 8.6905\n",
      "Epoch 62/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.9948\n",
      " - 0s - loss: 3.2931 - val_loss: 8.4941\n",
      "Epoch 63/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 15.2795\n",
      " - 0s - loss: 3.2667 - val_loss: 8.5353\n",
      "Epoch 64/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.9308\n",
      " - 0s - loss: 3.2770 - val_loss: 8.5687\n",
      "Epoch 65/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 14.9309\n",
      " - 0s - loss: 3.2834 - val_loss: 8.5723\n",
      "Epoch 66/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.9796\n",
      " - 0s - loss: 3.2076 - val_loss: 8.6364\n",
      "Epoch 67/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 14.8030\n",
      " - 0s - loss: 3.2019 - val_loss: 8.5880\n",
      "Epoch 68/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 15.2792\n",
      " - 0s - loss: 3.1945 - val_loss: 8.4573\n",
      "Epoch 69/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 14.9811\n",
      " - 0s - loss: 3.1966 - val_loss: 8.6877\n",
      "Epoch 70/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 14.8255\n",
      " - 0s - loss: 3.1208 - val_loss: 8.4504\n",
      "Epoch 71/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 14.8774\n",
      " - 0s - loss: 3.0607 - val_loss: 8.4743\n",
      "Epoch 72/500\n",
      "102/102 [==============================] - 0s 76us/sample - loss: 14.9354\n",
      " - 0s - loss: 3.0930 - val_loss: 8.5747\n",
      "Epoch 73/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 14.8782\n",
      " - 0s - loss: 3.0495 - val_loss: 8.6012\n",
      "Epoch 74/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.8918\n",
      " - 0s - loss: 3.0553 - val_loss: 8.5750\n",
      "Epoch 75/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.8895\n",
      " - 0s - loss: 3.0142 - val_loss: 8.6463\n",
      "Epoch 76/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.9549\n",
      " - 0s - loss: 3.0588 - val_loss: 8.4500\n",
      "Epoch 77/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 14.9914\n",
      " - 0s - loss: 3.0263 - val_loss: 8.4915\n",
      "Epoch 78/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 14.9233\n",
      " - 0s - loss: 3.0473 - val_loss: 8.5274\n",
      "Epoch 79/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.7537\n",
      " - 0s - loss: 3.0210 - val_loss: 8.5660\n",
      "Epoch 80/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 14.8754\n",
      " - 0s - loss: 2.9797 - val_loss: 8.5734\n",
      "Epoch 81/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.8616\n",
      " - 0s - loss: 2.9903 - val_loss: 8.6064\n",
      "Epoch 82/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.9105\n",
      " - 0s - loss: 2.9774 - val_loss: 8.5638\n",
      "Epoch 83/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.8363\n",
      " - 0s - loss: 2.9582 - val_loss: 8.6094\n",
      "Epoch 84/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.8565\n",
      " - 0s - loss: 2.9355 - val_loss: 8.5808\n",
      "Epoch 85/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 14.8866\n",
      " - 0s - loss: 2.9335 - val_loss: 8.5527\n",
      "Epoch 86/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 14.8587\n",
      " - 0s - loss: 2.9399 - val_loss: 8.5801\n",
      "Epoch 87/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 14.9084\n",
      " - 0s - loss: 2.9298 - val_loss: 8.4706\n",
      "Epoch 88/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.8811\n",
      " - 0s - loss: 2.9108 - val_loss: 8.5490\n",
      "Epoch 89/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.8512\n",
      " - 0s - loss: 2.9126 - val_loss: 8.5250\n",
      "Epoch 90/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.8583\n",
      " - 0s - loss: 2.9187 - val_loss: 8.6089\n",
      "Epoch 91/500\n",
      "102/102 [==============================] - 0s 98us/sample - loss: 14.8735\n",
      " - 0s - loss: 2.9005 - val_loss: 8.5791\n",
      "Epoch 92/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.8440\n",
      " - 0s - loss: 2.9055 - val_loss: 8.6098\n",
      "Epoch 93/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.8606\n",
      " - 0s - loss: 2.8848 - val_loss: 8.5703\n",
      "Epoch 94/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 14.8691\n",
      " - 0s - loss: 2.8791 - val_loss: 8.5533\n",
      "Epoch 95/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.8460\n",
      " - 0s - loss: 2.8782 - val_loss: 8.5916\n",
      "Epoch 96/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.8651\n",
      " - 0s - loss: 2.8760 - val_loss: 8.5635\n",
      "Epoch 97/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.8362\n",
      " - 0s - loss: 2.8728 - val_loss: 8.5893\n",
      "Epoch 98/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 14.8449\n",
      " - 0s - loss: 2.8745 - val_loss: 8.5629\n",
      "Epoch 99/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.8399\n",
      " - 0s - loss: 2.8716 - val_loss: 8.5620\n",
      "Epoch 100/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.8473\n",
      " - 0s - loss: 2.8685 - val_loss: 8.5732\n",
      "Epoch 101/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 14.8500\n",
      " - 0s - loss: 2.8548 - val_loss: 8.5678\n",
      "Epoch 102/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 14.8369\n",
      " - 0s - loss: 2.8545 - val_loss: 8.5829\n",
      "Epoch 103/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.8321\n",
      " - 0s - loss: 2.8516 - val_loss: 8.5816\n",
      "Epoch 104/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.8452\n",
      " - 0s - loss: 2.8591 - val_loss: 8.5880\n",
      "Epoch 105/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 14.8234\n",
      " - 0s - loss: 2.8484 - val_loss: 8.5886\n",
      "Epoch 106/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 14.8266\n",
      " - 0s - loss: 2.8508 - val_loss: 8.5826\n",
      "Epoch 107/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.8303\n",
      " - 0s - loss: 2.8511 - val_loss: 8.5966\n",
      "Epoch 108/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.8250\n",
      " - 0s - loss: 2.8504 - val_loss: 8.5638\n",
      "Epoch 109/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 14.8390\n",
      " - 0s - loss: 2.8492 - val_loss: 8.5851\n",
      "Epoch 110/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 14.8460\n",
      " - 0s - loss: 2.8466 - val_loss: 8.5827\n",
      "Epoch 111/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.8443\n",
      " - 0s - loss: 2.8408 - val_loss: 8.5800\n",
      "Epoch 112/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.8400\n",
      " - 0s - loss: 2.8370 - val_loss: 8.5758\n",
      "Epoch 113/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.8426\n",
      " - 0s - loss: 2.8382 - val_loss: 8.5826\n",
      "Epoch 114/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 14.8398\n",
      " - 0s - loss: 2.8378 - val_loss: 8.5716\n",
      "Epoch 115/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.8327\n",
      " - 0s - loss: 2.8385 - val_loss: 8.5892\n",
      "Epoch 116/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 14.8350\n",
      " - 0s - loss: 2.8345 - val_loss: 8.5758\n",
      "Epoch 117/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.8471\n",
      " - 0s - loss: 2.8429 - val_loss: 8.5673\n",
      "Epoch 118/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 0s 78us/sample - loss: 14.8439\n",
      " - 0s - loss: 2.8354 - val_loss: 8.5702\n",
      "Epoch 119/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 14.8383\n",
      " - 0s - loss: 2.8330 - val_loss: 8.5776\n",
      "Epoch 120/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 14.8403\n",
      " - 0s - loss: 2.8358 - val_loss: 8.5737\n",
      "Epoch 121/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 14.8392\n",
      " - 0s - loss: 2.8305 - val_loss: 8.5755\n",
      "Epoch 122/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 14.8358\n",
      " - 0s - loss: 2.8312 - val_loss: 8.5767\n",
      "Epoch 123/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 14.8425\n",
      " - 0s - loss: 2.8308 - val_loss: 8.5794\n",
      "Epoch 124/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 14.8376\n",
      " - 0s - loss: 2.8293 - val_loss: 8.5740\n",
      "Epoch 125/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.8358\n",
      " - 0s - loss: 2.8316 - val_loss: 8.5824\n",
      "Epoch 126/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 14.8389\n",
      " - 0s - loss: 2.8301 - val_loss: 8.5708\n",
      "Epoch 127/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.8409\n",
      " - 0s - loss: 2.8325 - val_loss: 8.5720\n",
      "Epoch 128/500\n",
      "102/102 [==============================] - 0s 78us/sample - loss: 14.8383\n",
      " - 0s - loss: 2.8288 - val_loss: 8.5752\n",
      "Epoch 129/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.8378\n",
      " - 0s - loss: 2.8311 - val_loss: 8.5717\n",
      "Epoch 130/500\n",
      "102/102 [==============================] - 0s 88us/sample - loss: 14.8392\n",
      " - 0s - loss: 2.8304 - val_loss: 8.5730\n",
      "dict_keys(['loss', 'val_loss', 'lr'])\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 64)                896       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 5,121\n",
      "Trainable params: 5,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "最优test_mse为12.095\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "import pdb\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "#导入数据集\n",
    "train_data = pd.read_csv('train.csv',header=0)\n",
    "#删除异常点数据\n",
    "i_=[]\n",
    "for i in range(len(train_data)):\n",
    "    if train_data['MEDV'][i]== 50:\n",
    "        i_.append(i)\n",
    "for i in range(len(i_)):\n",
    "    train_data=train_data.drop(i_[i])\n",
    "train_y = train_data['MEDV']\n",
    "train_x = train_data.drop('MEDV', axis = 1)\n",
    "#数据归一化\n",
    "test_data = pd.read_csv('test.csv')\n",
    "test_y = pd.read_csv('submission.csv')\n",
    "test_y = test_y['MEDV']\n",
    "test_x = test_data.drop('id', axis = 1) \n",
    "#归一化处理\n",
    "mean = train_x.mean(axis=0)\n",
    "std = train_x.std(axis=0)\n",
    "train_x = (train_x - mean) / std\n",
    "test_x = (test_x - mean) / std\n",
    "#数组初始化\n",
    "test_list=[]#每个epoch对测试集的mse\n",
    "all_val_mse = []#每个epoch对验证集的mse\n",
    "all_test_mse = [15]#第i折最后对测试集的mse 作为最好模型评估\n",
    "#输出路径初始化\n",
    "path = r'../output/logs/'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "    print(\"创建目录\")\n",
    "else:\n",
    "    print(\"目录已存在\")\n",
    "#回调加动态学习率、评价指标、保存最好模型\n",
    "class MyCallback(keras.callbacks.Callback):\n",
    "    def __init__(self,training_data,validation_data,testing_data):       \n",
    "        self.x = training_data[0]\n",
    "        self.y = training_data[1]\n",
    "        self.xx = validation_data[0]\n",
    "        self.yy = validation_data[1]\n",
    "        self.xxx = testing_data[0]\n",
    "        self.yyy = testing_data[1]\n",
    "        #pdb.set_trace()\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "    def on_train_end(self, epoch,logs={}):\n",
    "        return\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        test_mse_score = model.evaluate(self.xxx, self.yyy); #评估模型\n",
    "        if test_mse_score < 20:\n",
    "            test_list.append(test_mse_score);\n",
    "            if test_mse_score < all_test_mse[len(all_test_mse)-1]:\n",
    "                model.save(os.path.join(path,'best_model.h5'))\n",
    "                all_test_mse.append(test_mse_score)\n",
    "                print(\"目前最优test_mse为%.3f\" %test_mse_score)\n",
    "        return\n",
    "\n",
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(64,activation=tf.nn.relu,input_shape=(train_x.shape[1],)),\n",
    "#        Dropout(0.1),\n",
    "        keras.layers.Dense(64,activation=tf.nn.relu),\n",
    "#        Dropout(0.1),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    \n",
    "    model.compile(loss = 'mse',\n",
    "                 optimizer = optimizer)\n",
    "    return model\n",
    "\n",
    "k=4\n",
    "num_val_samples = len(train_x) // k\n",
    "num_epochs = 500\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, mode='auto')\n",
    "eraly_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=80, verbose=0)\n",
    "for i in range(k):\n",
    "    print('第', i+1 ,'折')\n",
    "    val_x = train_x[i*num_val_samples : (i+1)*num_val_samples]#划出验证集部分\n",
    "    val_y = train_y[i*num_val_samples : (i+1)*num_val_samples]\n",
    "    \n",
    "    partial_train_x = np.concatenate([train_x[:i*num_val_samples],train_x[(i+1)*num_val_samples:]],axis=0)\n",
    "    partial_train_y = np.concatenate([train_y[:i*num_val_samples], train_y[(i+1)*num_val_samples:]],axis=0)\n",
    "    \n",
    "    model = build_model()\n",
    "    \n",
    "    history = model.fit(partial_train_x, partial_train_y, epochs = num_epochs, batch_size=8,verbose = 2,\n",
    "                        callbacks = [MyCallback(training_data=[partial_train_x,partial_train_y],validation_data=[val_x,val_y],\n",
    "                                               testing_data=[test_x,test_y]),\n",
    "                                     reduce_lr,\n",
    "                                     eraly_stopping],\n",
    "                        validation_data = (val_x,val_y) )\n",
    "\n",
    "    print(history.history.keys())\n",
    "    #pdb.set_trace()\n",
    "    val_loss = history.history['val_loss']#记录验证集数据\n",
    "    all_val_mse.append(val_loss[len(val_loss)-1])\n",
    "    #test_mse_score = model.evaluate(test_x, test_y) #评估模型\n",
    "    #all_test_mse.append(test_mse_score)\n",
    "    #print('test_mse = ',test_mse_score)\n",
    "model.summary()\n",
    "print(\"最优test_mse为%.3f\" %all_test_mse[len(all_test_mse)-1])\n",
    "np.save(os.path.join(path,'all_val_mse.npy'),all_val_mse)\n",
    "np.save(os.path.join(path,'all_test_mse.npy'),all_test_mse)\n",
    "np.save(os.path.join(path,'test_list.npy'),test_list)\n",
    "y_pred = model.predict(test_x ,batch_size = 1)\n",
    "df = pd.DataFrame(y_pred)\n",
    "df.to_csv(os.path.join(path,'ownsubmissionAdam.csv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.447029772493028, 8.511755852968601, 5.857898896502466, 8.573006957020496]\n",
      "[15, 14.859270348268396, 14.570700860490986, 14.321835499183804, 13.81933707816928, 13.718132542628869, 13.514961546542597, 13.467071154538322, 13.446634554395489, 13.37276073998096, 13.32295433212729, 13.038697149239335, 12.963447982189702, 12.856189863354553, 12.822249417211495, 12.500402801177081, 12.451983208749809, 12.39808110629811, 12.384795717164582, 12.313365043378344, 12.196578904694201, 12.094548323575188]\n",
      "最优test_mse为12.095\n",
      "平均val_mse= (7.097)\n",
      "平均test_mse= (13.247)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nY0xUFiCMMcZEZQHCGGNMVP8fU2ucgexE+7UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (all_val_mse)\n",
    "print (all_test_mse)\n",
    "print(\"最优test_mse为%.3f\" %all_test_mse[len(all_test_mse)-1])\n",
    "print('平均val_mse= (%.3f)'% np.mean(np.array(all_val_mse)))\n",
    "print('平均test_mse= (%.3f)'% np.mean(np.array(all_test_mse)))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['loss'], 'r', label ='train_loss' )\n",
    "plt.plot(history.history['val_loss'], 'b', label='val_loss')\n",
    "plt.title('model loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('mse loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'][5:], 'r', label ='train_loss' )\n",
    "plt.plot(history.history['val_loss'][5:], 'b', label='val_loss')\n",
    "#plt.xticks([0,1000],['5','1000'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('mse loss')\n",
    "plt.legend()\n",
    "#plt.savefig('0.001,1000,8.png', dpi=200)\n",
    "plt.show()\n",
    "\n",
    "#plt.subplot(1,2,1)\n",
    "plt.plot(test_list, label ='mse' )#训练中模型每epoch对测试集mse指标\n",
    "plt.title('test mse')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('mse loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_headers = list(train_x.columns.values)\n",
    "#分析异常点\n",
    "for i in range(13):\n",
    "    plt.figure(figsize=(55,55))\n",
    "    plt.subplot(7,2,i+1)\n",
    "    plt.scatter(train_x.values[:,i],train_y,s=20)\n",
    "    plt.title(column_headers[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'/home/output/logs/'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "    print(\"创建目录\")\n",
    "else:\n",
    "    print(\"目录已存在\")\n",
    "hh=[123]\n",
    "numpy.save(os.path.join(path,'hhh.npy'),hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
